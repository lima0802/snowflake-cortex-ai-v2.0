{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cac8bf15",
   "metadata": {},
   "source": [
    "# ğŸ¤– Cortex Complete Interactive Tutorial\n",
    "## Learn by Doing: Snowflake LLM Text Generation\n",
    "\n",
    "**Author:** Li Ma  \n",
    "**Date:** February 24, 2026  \n",
    "**Project:** DIA v2.0 - Direct Marketing Analytics Intelligence\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You'll Learn\n",
    "\n",
    "This interactive notebook teaches you how to:\n",
    "1. âœ… Use Large Language Models (LLMs) in Snowflake\n",
    "2. âœ… Generate creative marketing content\n",
    "3. âœ… Summarize campaign data and reports\n",
    "4. âœ… Analyze sentiment in customer feedback\n",
    "5. âœ… Control creativity with temperature settings\n",
    "\n",
    "## ğŸ¯ Prerequisites\n",
    "\n",
    "- Docker containers running (`docker-compose up`)\n",
    "- Snowflake credentials configured in `.env` file\n",
    "- Cortex Complete enabled in your Snowflake account\n",
    "\n",
    "## ğŸ§  What is Cortex Complete?\n",
    "\n",
    "**Cortex Complete** is Snowflake's LLM service that generates text using AI models like:\n",
    "- **llama3-70b**: Best quality (70 billion parameters)\n",
    "- **mistral-large**: Great reasoning\n",
    "- **mistral-7b**: Fast and efficient\n",
    "\n",
    "**Use Cases:**\n",
    "- Write email subject lines\n",
    "- Summarize data and reports\n",
    "- Generate creative marketing content\n",
    "- Analyze sentiment\n",
    "- Answer questions (without accessing data)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ’¡ Tip:** Run each cell with `Shift + Enter` and experiment with different prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this notebook\n",
    "# Run this cell once to install dependencies\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    'structlog',\n",
    "    'python-dotenv',\n",
    "    'snowflake-snowpark-python'\n",
    "]\n",
    "\n",
    "print(\"ğŸ“¦ Installing required packages...\")\n",
    "for package in packages:\n",
    "    print(f\"   Installing {package}...\")\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"   âœ… {package} installed\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"   âŒ Failed to install {package}: {e}\")\n",
    "\n",
    "print(\"\\nâœ… Installation complete!\")\n",
    "print(\"âš ï¸  If this is the first install, please RESTART THE KERNEL:\")\n",
    "print(\"   Jupyter menu: Kernel â†’ Restart Kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Python paths and import libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Calculate the project paths dynamically\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "orchestrator_path = os.path.join(project_root, 'orchestrator')\n",
    "\n",
    "# Add paths for both local and Docker environments\n",
    "sys.path.insert(0, orchestrator_path)\n",
    "sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "print(f\"ğŸ“ Python paths added:\")\n",
    "print(f\"   Project Root: {project_root}\")\n",
    "print(f\"   Orchestrator: {orchestrator_path}\")\n",
    "\n",
    "# Verify orchestrator path exists\n",
    "if os.path.exists(orchestrator_path):\n",
    "    print(f\"   âœ… Orchestrator directory found\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Orchestrator directory NOT found at: {orchestrator_path}\")\n",
    "\n",
    "# Core Python libraries\n",
    "import json\n",
    "from typing import Dict, List, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Snowflake libraries\n",
    "from snowflake.snowpark import Session\n",
    "\n",
    "# Environment and logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Try to import custom logger with fallback\n",
    "try:\n",
    "    from utils.logging import get_logger\n",
    "    logger = get_logger(__name__)\n",
    "    print(f\"   âœ… Using custom structlog logger\")\n",
    "except ImportError as e:\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    print(f\"   âš ï¸  Using standard logging (utils.logging not found)\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"\\nâœ… All libraries imported successfully!\")\n",
    "print(f\"   Python version: {sys.version.split()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad6838",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Understanding the Response Data Model\n",
    "\n",
    "The `CompleteResponse` dataclass holds all information from an LLM generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d47b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CompleteResponse:\n",
    "    \"\"\"\n",
    "    Container for Cortex Complete (LLM) responses.\n",
    "    \n",
    "    Attributes:\n",
    "        prompt (str): The input prompt/question\n",
    "        completion (str): The generated text from LLM\n",
    "        model (str): Which model was used (e.g., llama3-70b)\n",
    "        metadata (Dict): Additional info (tokens, temperature, etc.)\n",
    "        error (str): Error message if something went wrong\n",
    "    \"\"\"\n",
    "    prompt: str\n",
    "    completion: Optional[str] = None\n",
    "    model: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary (useful for JSON APIs)\"\"\"\n",
    "        return {\n",
    "            \"prompt\": self.prompt,\n",
    "            \"completion\": self.completion,\n",
    "            \"model\": self.model,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"error\": self.error\n",
    "        }\n",
    "\n",
    "# Test it out!\n",
    "sample_response = CompleteResponse(\n",
    "    prompt=\"Write a catchy email subject line\",\n",
    "    completion=\"ğŸ‰ Limited Time: 50% Off Your Favorite Products!\",\n",
    "    model=\"llama3-70b\",\n",
    "    metadata={\"temperature\": 0.9, \"tokens\": 12}\n",
    ")\n",
    "\n",
    "print(\"âœ… CompleteResponse created!\")\n",
    "print(f\"   Prompt: {sample_response.prompt}\")\n",
    "print(f\"   Generated: {sample_response.completion}\")\n",
    "print(f\"   Model: {sample_response.model}\")\n",
    "print(f\"   Temperature: {sample_response.metadata['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb1742e",
   "metadata": {},
   "source": [
    "## ğŸ”§ Import CortexComplete Service\n",
    "\n",
    "Now let's import the `CortexComplete` class from the services module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CortexComplete service class\n",
    "try:\n",
    "    from services.cortex_complete import CortexComplete\n",
    "    print(\"âœ… CortexComplete class imported successfully!\")\n",
    "    print(\"   Ready to generate text with LLMs\")\n",
    "    print(\"\\nğŸ“Š Available Models:\")\n",
    "    for model in CortexComplete.AVAILABLE_MODELS:\n",
    "        print(f\"   â€¢ {model}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import CortexComplete: {e}\")\n",
    "    print(\"\\nğŸ’¡ Troubleshooting:\")\n",
    "    print(\"   1. Make sure you ran Cell 2 (path setup)\")\n",
    "    print(\"   2. Check that orchestrator/services/cortex_complete.py exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f1f21",
   "metadata": {},
   "source": [
    "## ğŸ¨ Example 1: Generate Marketing Content\n",
    "\n",
    "Let's use the LLM to generate creative marketing content. Higher temperature = more creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bf442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a marketing tagline\n",
    "with CortexComplete(model=\"llama3-70b\", temperature=0.9) as llm:\n",
    "    response = llm.complete(\n",
    "        \"Write a creative tagline for a data analytics platform that helps marketers understand email performance\"\n",
    "    )\n",
    "    \n",
    "    if response.error:\n",
    "        print(f\"âŒ Error: {response.error}\")\n",
    "    else:\n",
    "        print(\"âœ… Generated Tagline:\")\n",
    "        print(f\"\\n   {response.completion}\\n\")\n",
    "        print(f\"ğŸ“Š Metadata:\")\n",
    "        print(f\"   Model: {response.model}\")\n",
    "        print(f\"   Temperature: {response.metadata.get('temperature')}\")\n",
    "        print(f\"   Prompt Length: {response.metadata.get('prompt_length')} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237908f5",
   "metadata": {},
   "source": [
    "## ğŸ“ Example 2: Text Summarization\n",
    "\n",
    "Summarize long text into concise summaries. Lower temperature = more focused!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6060b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize campaign data\n",
    "long_text = \"\"\"\n",
    "The Q1 2026 email marketing campaign ran from January 1 to March 31.\n",
    "We sent 500,000 emails across 50 different customer segments.\n",
    "Overall open rate was 24.5%, which exceeded our 20% target.\n",
    "Click-through rate was 3.2%, slightly below our 3.5% target.\n",
    "We generated $125,000 in revenue, representing a 15% increase from Q4 2025.\n",
    "The top performing segments were Premium customers (35% open rate),\n",
    "and Recent purchasers (28% open rate).\n",
    "Mobile open rate was 18%, desktop was 32%.\n",
    "Best send time was Tuesday 10 AM EST.\n",
    "\"\"\"\n",
    "\n",
    "with CortexComplete(model=\"mistral-large\", temperature=0.3) as llm:\n",
    "    response = llm.summarize(long_text, max_length=30)\n",
    "    \n",
    "    if response.error:\n",
    "        print(f\"âŒ Error: {response.error}\")\n",
    "    else:\n",
    "        print(\"âœ… Summary Generated:\")\n",
    "        print(f\"\\n   {response.completion}\\n\")\n",
    "        print(f\"ğŸ“Š Comparison:\")\n",
    "        print(f\"   Original: {len(long_text.split())} words\")\n",
    "        print(f\"   Summary: {len(response.completion.split())} words\")\n",
    "        print(f\"   Reduction: {100 - (len(response.completion.split())/len(long_text.split())*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4be9c9",
   "metadata": {},
   "source": [
    "## âœ‰ï¸ Example 3: Generate Email Subject Lines\n",
    "\n",
    "Generate multiple creative subject lines for campaigns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate email subject lines\n",
    "with CortexComplete(model=\"llama3-70b\", temperature=0.9) as llm:\n",
    "    response = llm.generate_subject_lines(\n",
    "        campaign_info=\"Spring collection launch, 25% off all items, free shipping over $50, limited 3-day sale\",\n",
    "        count=5\n",
    "    )\n",
    "    \n",
    "    if response.error:\n",
    "        print(f\"âŒ Error: {response.error}\")\n",
    "    else:\n",
    "        print(\"âœ… Generated Subject Lines:\\n\")\n",
    "        print(response.completion)\n",
    "        print(f\"\\nğŸ’¡ Tip: Try different temperatures:\")\n",
    "        print(\"   â€¢ 0.9-1.0 = Very creative (best for marketing)\")\n",
    "        print(\"   â€¢ 0.5-0.7 = Balanced\")\n",
    "        print(\"   â€¢ 0.0-0.3 = Focused and consistent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3891c5",
   "metadata": {},
   "source": [
    "## ğŸ˜Š Example 4: Sentiment Analysis\n",
    "\n",
    "Analyze the sentiment of customer feedback or reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentiment of customer feedback\n",
    "feedback_samples = [\n",
    "    \"This email platform is absolutely amazing! Best decision we made.\",\n",
    "    \"The system crashed during our biggest campaign. Very disappointed.\",\n",
    "    \"It works fine, nothing special. Does what it needs to do.\",\n",
    "]\n",
    "\n",
    "with CortexComplete(model=\"mistral-7b\", temperature=0.2) as llm:\n",
    "    print(\"ğŸ” Sentiment Analysis Results:\\n\")\n",
    "    \n",
    "    for i, feedback in enumerate(feedback_samples, 1):\n",
    "        response = llm.analyze_sentiment(feedback)\n",
    "        \n",
    "        print(f\"{i}. Feedback: \\\"{feedback}\\\"\")\n",
    "        if response.error:\n",
    "            print(f\"   âŒ Error: {response.error}\")\n",
    "        else:\n",
    "            print(f\"   Result: {response.completion}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d14047",
   "metadata": {},
   "source": [
    "## ğŸ¯ Example 5: Compare Different Models\n",
    "\n",
    "Let's see how different models respond to the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ad64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models side-by-side\n",
    "prompt = \"Explain what email click-through rate (CTR) means in simple terms.\"\n",
    "\n",
    "models_to_test = [\"llama3-70b\", \"mistral-large\", \"mistral-7b\"]\n",
    "\n",
    "print(\"ğŸ“Š Comparing Model Responses:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\nğŸ¤– Model: {model_name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        with CortexComplete(model=model_name, temperature=0.5) as llm:\n",
    "            response = llm.complete(prompt)\n",
    "            \n",
    "            if response.error:\n",
    "                print(f\"âŒ Error: {response.error}\")\n",
    "            else:\n",
    "                print(response.completion)\n",
    "                print(f\"\\nLength: {len(response.completion.split())} words\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ Notice how different models have different styles and lengths!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d233b301",
   "metadata": {},
   "source": [
    "## ğŸŒ¡ï¸ Example 6: Temperature Experiment\n",
    "\n",
    "See how temperature affects creativity. Same prompt, different temperatures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92754b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "prompt = \"Write a short email subject line for a weekend flash sale\"\n",
    "temperatures = [0.0, 0.5, 1.0]\n",
    "\n",
    "print(\"ğŸŒ¡ï¸ Temperature Comparison:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with CortexComplete(model=\"llama3-70b\") as llm:\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nğŸŒ¡ï¸  Temperature: {temp}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        response = llm.complete(prompt, temperature=temp)\n",
    "        \n",
    "        if response.error:\n",
    "            print(f\"âŒ Error: {response.error}\")\n",
    "        else:\n",
    "            print(f\"{response.completion}\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nğŸ’¡ Observations:\")\n",
    "print(\"   â€¢ Temperature 0.0: Consistent, focused, predictable\")\n",
    "print(\"   â€¢ Temperature 0.5: Balanced creativity\")\n",
    "print(\"   â€¢ Temperature 1.0: Very creative, varied, unpredictable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c2c8e",
   "metadata": {},
   "source": [
    "## ğŸ“ Summary: What You Learned\n",
    "\n",
    "Congratulations! You've learned:\n",
    "\n",
    "âœ… **Cortex Complete Fundamentals**\n",
    "- How to use LLMs in Snowflake\n",
    "- Different models and their strengths\n",
    "- Temperature control for creativity\n",
    "\n",
    "âœ… **Practical Applications**\n",
    "- Generate marketing content\n",
    "- Summarize long text\n",
    "- Create email subject lines\n",
    "- Analyze sentiment\n",
    "- Compare model outputs\n",
    "\n",
    "âœ… **Python Programming**\n",
    "- Context managers (`with` statement)\n",
    "- Dataclasses for structured data\n",
    "- Error handling patterns\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "**Try These Experiments:**\n",
    "1. Generate subject lines for your own campaigns\n",
    "2. Summarize your actual campaign reports\n",
    "3. Test different models on the same task\n",
    "4. Adjust temperature to see creativity variations\n",
    "\n",
    "**Advanced Use Cases:**\n",
    "1. **Content Generation Pipeline**\n",
    "   - Generate â†’ Review â†’ Refine â†’ Deploy\n",
    "2. **Batch Processing**\n",
    "   - Loop through multiple prompts\n",
    "   - Save results to CSV/database\n",
    "3. **A/B Testing**\n",
    "   - Generate multiple variants\n",
    "   - Test which performs better\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”— Related Resources\n",
    "\n",
    "- **Documentation:** `guides/02_STEP_2.1_CORTEX_SERVICES.md`\n",
    "- **Service Code:** `orchestrator/services/cortex_complete.py`\n",
    "- **Other Notebooks:**\n",
    "  - `cortex_analyst_interactive.ipynb` - SQL generation\n",
    "  - `cortex_search_interactive.ipynb` - Semantic search\n",
    "  - `cortex_ml_interactive.ipynb` - Forecasting & anomalies\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Differences: Analyst vs Complete\n",
    "\n",
    "**Cortex Analyst:**\n",
    "- â“ Question â†’ ğŸ” SQL â†’ ğŸ“Š Real Data\n",
    "- Use when: You need actual data from database\n",
    "\n",
    "**Cortex Complete:**\n",
    "- ğŸ’¬ Prompt â†’ ğŸ¤– LLM â†’ ğŸ“ Generated Text\n",
    "- Use when: You need creative content or summaries\n",
    "\n",
    "**Use Together:**\n",
    "```python\n",
    "# Get data with Analyst\n",
    "analyst = CortexAnalyst()\n",
    "data = analyst.send_message(\"What was revenue last month?\")\n",
    "\n",
    "# Summarize with Complete\n",
    "llm = CortexComplete()\n",
    "summary = llm.summarize(str(data.results), max_length=50)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Status:** âœ… Tutorial Complete  \n",
    "**Next:** Try `cortex_search_interactive.ipynb` for semantic search!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
