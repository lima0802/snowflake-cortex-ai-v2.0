"""
Cortex Analyst Service Wrapper - DIA v2.0
==========================================

This module provides a Python wrapper for Snowflake Cortex Analyst,
which converts natural language questions into SQL queries and executes them.

Key Concepts:
- Cortex Analyst uses a semantic model (YAML) to understand your data schema
- It generates SQL from natural language questions
- Returns both the SQL query and the results

Learning Resources:
- https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst
- https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/using-api

Author: Li Ma
Date: February 22, 2026
docker exec dia-orchestrator python services/cortex_analyst.py
"""

import os
import sys
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import json

from snowflake.snowpark import Session
from dotenv import load_dotenv

# Add parent directory to Python path for proper imports
# This allows 'from utils.logging import ...' to work regardless of where script is run from
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import your logging utility (you already have this!)
from utils.logging import get_logger

# Load environment variables from .env file
load_dotenv()

# Initialize logger for this module
logger = get_logger(__name__)


# ==============================================================================
# DATA MODELS
# ==============================================================================
# These dataclasses define the structure of our data
# Think of them as "blueprints" for our objects

@dataclass
class AnalystResponse:
    """
    Represents a response from Cortex Analyst.
    
    This is like a container that holds all the information we get back
    from Cortex Analyst when we ask it a question.
    
    Attributes:
        query (str): The original natural language question from the user
        sql (str): The SQL query generated by Cortex Analyst
        results (List[Dict]): The data returned from executing the SQL
        metadata (Dict): Additional info (row count, execution time, etc.)
        error (Optional[str]): Error message if something went wrong
    
    Example:
        response = AnalystResponse(
            query="What was the click rate last month?",
            sql="SELECT AVG(CLICK_RATE) FROM VW_SFMC_EMAIL_PERFORMANCE...",
            results=[{"AVG_CLICK_RATE": 3.2}],
            metadata={"row_count": 1}
        )
    """
    query: str
    sql: Optional[str] = None
    results: Optional[List[Dict[str, Any]]] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert response to dictionary (useful for JSON APIs)"""
        return {
            "query": self.query,
            "sql": self.sql,
            "results": self.results,
            "metadata": self.metadata,
            "error": self.error
        }


# ==============================================================================
# MAIN SERVICE CLASS
# ==============================================================================

class CortexAnalyst:
    """
    Python wrapper for Snowflake Cortex Analyst service.
    
    This class handles all interactions with Cortex Analyst:
    - Connecting to Snowflake
    - Sending natural language questions
    - Receiving SQL and results
    - Error handling and logging
    
    Usage Example:
        # Create an instance
        analyst = CortexAnalyst()
        
        # Ask a question
        response = analyst.send_message("What was the average open rate last week?")
        
        # Access the results
        print(f"SQL: {response.sql}")
        print(f"Results: {response.results}")
    """
    
    def __init__(
        self,
        semantic_model_file: str = "semantic.yaml",
        stage_name: str = "SEMANTIC_MODELS"
    ):
        """
        Initialize the Cortex Analyst wrapper.
        
        Args:
            semantic_model_file (str): Name of your semantic model YAML file
            stage_name (str): Snowflake stage where the semantic model is stored
        
        What happens here:
        1. Loads configuration from environment variables (.env file)
        2. Creates a connection session to Snowflake
        3. Prepares the path to your semantic model
        """
        # Store configuration
        self.semantic_model_file = semantic_model_file
        self.stage_name = stage_name
        
        # Get Snowflake credentials from environment variables
        # These were set in your .env file
        self.account = os.getenv("SNOWFLAKE_ACCOUNT")
        self.user = os.getenv("SNOWFLAKE_USER")
        self.password = os.getenv("SNOWFLAKE_PASSWORD")
        self.database = os.getenv("SNOWFLAKE_DATABASE")
        self.schema = os.getenv("SNOWFLAKE_SCHEMA")
        self.warehouse = os.getenv("SNOWFLAKE_WAREHOUSE")
        self.role = os.getenv("SNOWFLAKE_ROLE")
        
        # Initialize Snowflake session (connection will be created on demand)
        self._session: Optional[Session] = None
        
        # Log initialization
        logger.info(
            "CortexAnalyst initialized",
            database=self.database,
            schema=self.schema,
            semantic_model=f"@{stage_name}/{semantic_model_file}"
        )
    
    # --------------------------------------------------------------------------
    # CONNECTION MANAGEMENT
    # --------------------------------------------------------------------------
    
    def _get_session(self) -> Session:
        """
        Get or create a Snowflake session (lazy loading pattern).
        
        Why "lazy loading"?
        Instead of connecting immediately when we create the CortexAnalyst object,
        we wait until we actually need the connection. This saves resources!
        
        Returns:
            Session: Active Snowflake session
        
        Raises:
            Exception: If connection fails
        """
        # If we already have a session, reuse it
        if self._session is not None:
            return self._session
        
        try:
            # Create connection parameters dictionary
            connection_params = {
                "account": self.account,
                "user": self.user,
                "password": self.password,
                "database": self.database,
                "schema": self.schema,
                "warehouse": self.warehouse,
                "role": self.role
            }
            
            # Create the session (connect to Snowflake)
            logger.info("Creating Snowflake session...")
            self._session = Session.builder.configs(connection_params).create()
            
            logger.info(
                "Snowflake session created successfully",
                account=self.account,
                database=self.database
            )
            
            return self._session
            
        except Exception as e:
            logger.error(f"Failed to create Snowflake session: {e}")
            raise
    
    def close(self):
        """
        Close the Snowflake session.
        
        Always call this when you're done to free up resources!
        Good practice: Use "with" statement or try/finally block.
        
        Example:
            analyst = CortexAnalyst()
            try:
                response = analyst.send_message("Show me metrics")
            finally:
                analyst.close()  # Always cleanup!
        """
        if self._session:
            self._session.close()
            self._session = None
            logger.info("Snowflake session closed")
    
    # --------------------------------------------------------------------------
    # CORE FUNCTIONALITY
    # --------------------------------------------------------------------------
    
    def send_message(
        self,
        query: str,
        conversation_id: Optional[str] = None
    ) -> AnalystResponse:
        """
        Send a natural language question to Cortex Analyst.
        
        This is the main method you'll use! It:
        1. Takes your natural language question
        2. Sends it to Cortex Analyst
        3. Gets back SQL and executes it
        4. Returns the results in a nice format
        
        Args:
            query (str): Your natural language question
                Examples:
                - "What was the average open rate last month?"
                - "Show me click rates by market"
                - "Which campaigns had bounce rate above 5%?"
            
            conversation_id (Optional[str]): ID for multi-turn conversations
                (useful for follow-up questions - Cortex remembers context!)
        
        Returns:
            AnalystResponse: Contains SQL, results, and metadata
        
        Example:
            analyst = CortexAnalyst()
            
            # Ask a question
            response = analyst.send_message(
                "What was total emails sent in January 2026?"
            )
            
            # Check if successful
            if response.error:
                print(f"Error: {response.error}")
            else:
                print(f"SQL Generated: {response.sql}")
                print(f"Results: {response.results}")
        """
        logger.info("Processing natural language query", query=query)
        
        try:
            # Get Snowflake connection
            session = self._get_session()
            
            # Build the semantic model reference
            # Format: @DATABASE.SCHEMA.STAGE_NAME/file.yaml
            semantic_model_ref = (
                f"@{self.database}.{self.schema}.{self.stage_name}/"
                f"{self.semantic_model_file}"
            )
            
            # --------------------------------------------------------------
            # CORTEX ANALYST API CALL
            # --------------------------------------------------------------
            # This is the magic! We call Snowflake's Cortex Analyst function
            # 
            # SNOWFLAKE.CORTEX.ANALYST() function:
            # - First parameter: Your question (natural language)
            # - Second parameter: Path to semantic model
            # - Third parameter (optional): Conversation ID for context
            
            if conversation_id:
                # Multi-turn conversation (remembers previous questions)
                sql_query = f"""
                    SELECT SNOWFLAKE.CORTEX.ANALYST(
                        '{self._escape_quotes(query)}',
                        '{semantic_model_ref}',
                        '{conversation_id}'
                    ) AS response
                """
            else:
                # Single question (no conversation context)
                sql_query = f"""
                    SELECT SNOWFLAKE.CORTEX.ANALYST(
                        '{self._escape_quotes(query)}',
                        '{semantic_model_ref}'
                    ) AS response
                """
            
            logger.debug("Executing Cortex Analyst query", sql=sql_query)
            
            # Execute the query
            result = session.sql(sql_query).collect()
            
            # Parse the response from Cortex Analyst
            # The result comes back as JSON, so we need to parse it
            if result and len(result) > 0:
                response_json = result[0]['RESPONSE']
                
                # Convert from JSON string to Python dict
                if isinstance(response_json, str):
                    response_data = json.loads(response_json)
                else:
                    response_data = response_json
                
                logger.debug("Parsed Cortex Analyst response", response=response_data)
                
                # Extract the components from the response
                # Cortex Analyst returns: SQL query, results, and metadata
                generated_sql = response_data.get('sql', None)
                query_results = response_data.get('results', [])
                metadata = response_data.get('metadata', {})
                
                # If SQL was generated, we can execute it to get fresh results
                # (or use the results that Cortex already executed for us)
                if generated_sql and not query_results:
                    logger.info("Executing generated SQL", sql=generated_sql)
                    query_results = self._execute_sql(generated_sql)
                
                # Create successful response
                response = AnalystResponse(
                    query=query,
                    sql=generated_sql,
                    results=query_results,
                    metadata=metadata or {"row_count": len(query_results)}
                )
                
                logger.info(
                    "Query processed successfully",
                    query=query,
                    row_count=len(query_results) if query_results else 0
                )
                
                return response
            
            else:
                # No results returned - something went wrong
                error_msg = "No response from Cortex Analyst"
                logger.warning(error_msg, query=query)
                
                return AnalystResponse(
                    query=query,
                    error=error_msg
                )
        
        except Exception as e:
            # Handle any errors that occurred
            error_msg = f"Error processing query: {str(e)}"
            logger.error(error_msg, query=query, exception=str(e))
            
            return AnalystResponse(
                query=query,
                error=error_msg
            )
    
    # --------------------------------------------------------------------------
    # HELPER METHODS
    # --------------------------------------------------------------------------
    
    def _escape_quotes(self, text: str) -> str:
        """
        Escape single quotes in text for SQL injection safety.
        
        Why is this important?
        If a user asks: "Show me John's campaigns"
        We need to escape the apostrophe so SQL doesn't break:
        "Show me John''s campaigns" (double quote in SQL)
        
        Args:
            text (str): Text that might contain quotes
        
        Returns:
            str: Text with quotes properly escaped
        """
        return text.replace("'", "''")
    
    def _execute_sql(self, sql: str) -> List[Dict[str, Any]]:
        """
        Execute a SQL query and return results as list of dictionaries.
        
        This is useful when you have a SQL query and want to run it
        against your Snowflake database.
        
        Args:
            sql (str): The SQL query to execute
        
        Returns:
            List[Dict[str, Any]]: Results as list of row dictionaries
        
        Example Result:
            [
                {"MARKET": "UK", "OPEN_RATE": 22.5},
                {"MARKET": "Germany", "OPEN_RATE": 19.8}
            ]
        """
        try:
            session = self._get_session()
            
            # Execute the query
            result = session.sql(sql).collect()
            
            # Convert Snowflake rows to dictionaries
            results = []
            for row in result:
                # Convert Row object to dictionary
                row_dict = row.asDict()
                results.append(row_dict)
            
            logger.debug(f"SQL executed successfully, {len(results)} rows returned")
            return results
            
        except Exception as e:
            logger.error(f"Failed to execute SQL: {e}", sql=sql)
            raise
    
    def verify_semantic_model(self) -> Dict[str, Any]:
        """
        Verify that the semantic model file exists in the Snowflake stage.
        
        This is a useful diagnostic method to check your setup!
        Call this if you're having issues to verify everything is configured correctly.
        
        Returns:
            Dict with verification results:
            {
                "exists": True/False,
                "file_name": "semantic.yaml",
                "stage_path": "@DATABASE.SCHEMA.STAGE",
                "file_size": 12345,
                "last_modified": "2026-02-22..."
            }
        
        Example:
            analyst = CortexAnalyst()
            verification = analyst.verify_semantic_model()
            
            if verification["exists"]:
                print("‚úÖ Semantic model found!")
            else:
                print("‚ùå Semantic model not found - check deployment")
        """
        try:
            session = self._get_session()
            
            # List files in the stage
            stage_path = f"@{self.database}.{self.schema}.{self.stage_name}"
            list_sql = f"LIST {stage_path}"
            
            logger.info("Checking semantic model existence", stage=stage_path)
            
            result = session.sql(list_sql).collect()
            
            # Look for our semantic model file
            for row in result:
                file_name = row['name']
                if self.semantic_model_file in file_name:
                    verification = {
                        "exists": True,
                        "file_name": file_name,
                        "stage_path": stage_path,
                        "file_size": row['size'],
                        "last_modified": str(row['last_modified'])
                    }
                    logger.info("Semantic model verified", **verification)
                    return verification
            
            # File not found
            logger.warning(
                "Semantic model not found in stage",
                stage=stage_path,
                expected_file=self.semantic_model_file
            )
            
            return {
                "exists": False,
                "file_name": self.semantic_model_file,
                "stage_path": stage_path,
                "error": f"File '{self.semantic_model_file}' not found in {stage_path}"
            }
            
        except Exception as e:
            logger.error(f"Failed to verify semantic model: {e}")
            return {
                "exists": False,
                "error": str(e)
            }
    
    # --------------------------------------------------------------------------
    # CONTEXT MANAGER SUPPORT (for "with" statement)
    # --------------------------------------------------------------------------
    
    def __enter__(self):
        """Enable use with 'with' statement (context manager)"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Automatically close connection when exiting 'with' block"""
        self.close()


# ==============================================================================
# EXAMPLE USAGE (for learning and testing)
# ==============================================================================

if __name__ == "__main__":
    """
    Example usage of CortexAnalyst class.
    
    Run this file directly to test:
        python orchestrator/services/cortex_analyst.py
    
    Or from Docker:
        docker exec dia-orchestrator python services/cortex_analyst.py
    """
    
    print("=" * 70)
    print("CORTEX ANALYST SERVICE - EXAMPLE USAGE")
    print("=" * 70)
    
    # Example 1: Basic usage
    print("\nüìù Example 1: Basic Query")
    print("-" * 70)
    
    analyst = CortexAnalyst()
    
    try:
        # Ask a simple question
        response = analyst.send_message("What was the total emails sent last month?")
        
        if response.error:
            print(f"‚ùå Error: {response.error}")
        else:
            print(f"‚úÖ Query: {response.query}")
            print(f"üìä SQL Generated:\n{response.sql}")
            print(f"üìà Results: {response.results}")
            print(f"‚ÑπÔ∏è  Metadata: {response.metadata}")
    
    finally:
        analyst.close()
    
    # Example 2: Using context manager (recommended!)
    print("\nüìù Example 2: Using Context Manager")
    print("-" * 70)
    
    with CortexAnalyst() as analyst:
        # Verify semantic model first
        verification = analyst.verify_semantic_model()
        print(f"üîç Semantic Model Check: {verification}")
        
        # Ask a question
        response = analyst.send_message("Show me click rates by market")
        print(f"‚úÖ Results: {response.results}")
    
    # Connection automatically closed!
    
    print("\n" + "=" * 70)
    print("‚úÖ Examples complete! Check the code to learn how it works.")
    print("=" * 70)
